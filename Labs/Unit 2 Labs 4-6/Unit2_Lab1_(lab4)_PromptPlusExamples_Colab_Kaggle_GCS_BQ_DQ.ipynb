{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i37AnNNStgw7"
      },
      "source": [
        "# MGMT 467 — Prompt-Driven Lab (with Commented Examples)\n",
        "## Kaggle ➜ Google Cloud Storage ➜ BigQuery ➜ Data Quality (DQ)\n",
        "\n",
        "**How to use this notebook**\n",
        "- Each section gives you a **Build Prompt** to paste into Gemini/Vertex AI (or Gemini in Colab).\n",
        "- Below each prompt, you’ll see a **commented example** of what a good LLM answer might look like.\n",
        "- **Do not** just uncomment and run. Use the prompt to generate your own code, then compare to the example.\n",
        "- After every step, run the **Verification Prompt**, and write the **Reflection** in Markdown.\n",
        "\n",
        "> Goal today: Download the Netflix dataset (Kaggle) → Stage on GCS → Load into BigQuery → Run DQ profiling (missingness, duplicates, outliers, anomaly flags).\n"
      ],
      "id": "i37AnNNStgw7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2NvQefFtgw9"
      },
      "source": [
        "### Academic integrity & LLM usage\n",
        "- Use the prompts here to generate your own code cells.\n",
        "- Read concept notes and write the reflection answers in your own words.\n",
        "- Keep credentials out of code. Upload `kaggle.json` when asked.\n"
      ],
      "id": "f2NvQefFtgw9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92JfUv2stgw-"
      },
      "source": [
        "## Learning objectives\n",
        "1) Explain **why** we stage data in GCS and load it to BigQuery.  \n",
        "2) Build an **idempotent**, auditable pipeline.  \n",
        "3) Diagnose **missingness**, **duplicates**, and **outliers** and justify cleaning choices.  \n",
        "4) Connect DQ decisions to **business/ML impact**.\n"
      ],
      "id": "92JfUv2stgw-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ki621jC9tgw-"
      },
      "source": [
        "## 0) Environment setup — What & Why\n",
        "Authenticate Colab to Google Cloud so we can use `gcloud`, GCS, and BigQuery. Set **PROJECT_ID** and **REGION** once for consistency (cost/latency)."
      ],
      "id": "Ki621jC9tgw-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "la0Lco3ztgw-"
      },
      "source": [
        "### Build Prompt (paste to LLM)\n",
        "You are my cloud TA. Generate a single **Colab code cell** that:\n",
        "1) Authenticates to Google Cloud in Colab,  \n",
        "2) Prompts for `PROJECT_ID` via `input()` and sets `REGION=\"us-central1\"` (editable),  \n",
        "3) Exports `GOOGLE_CLOUD_PROJECT`,  \n",
        "4) Runs `gcloud config set project $GOOGLE_CLOUD_PROJECT`,  \n",
        "5) Prints both values. Add 2–3 comments explaining what/why.\n",
        "End with a comment: `# Done: Auth + Project/Region set`.\n"
      ],
      "id": "la0Lco3ztgw-"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K57WeUIAtgw-",
        "outputId": "d73218e2-9e7f-48a1-884a-bc7b8ebb84ac"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your GCP Project ID: mgmt467-project1\n",
            "Project: mgmt467-project1 | Region: us-central1\n",
            "Updated property [core/project].\n",
            "mgmt467-project1\n"
          ]
        }
      ],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import os\n",
        "PROJECT_ID = input(\"Enter your GCP Project ID: \").strip()\n",
        "REGION = \"us-central1\"  # keep consistent; change if instructed\n",
        "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
        "print(\"Project:\", PROJECT_ID, \"| Region:\", REGION)\n",
        "\n",
        "# Set active project for gcloud/BigQuery CLI\n",
        "!gcloud config set project $GOOGLE_CLOUD_PROJECT\n",
        "!gcloud config get-value project\n",
        "# Done: Auth + Project/Region set"
      ],
      "id": "K57WeUIAtgw-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGkOge_Ftgw_"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a short cell that prints the active project using `gcloud config get-value project` and echoes the `REGION` you set.\n"
      ],
      "id": "hGkOge_Ftgw_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jsftw-Xtgw_"
      },
      "source": [
        "**Reflection:** Why do we set `PROJECT_ID` and `REGION` at the top? What can go wrong if we don’t?\n",
        "\n",
        "*We set `PROJECT_ID` and `REGION` at the top of the notebook so that all google cloud operations know which project and geographic location to use while executing the code. These variables ensure that resources like datasets, and models are created under the correct billing account and in a consistent region. If they aren’t set, API calls can fail due to missing project information, or resources might be created in the wrong region, leading to errors, higher costs, or access issues. Defining them once at the start helps maintain consistency, reproducibility, and smooth execution throughout the project.*"
      ],
      "id": "1jsftw-Xtgw_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ChY39vqtgw_"
      },
      "source": [
        "## 1) Kaggle API — What & Why\n",
        "Use Kaggle CLI for reproducible downloads. Store `kaggle.json` at `~/.kaggle/kaggle.json` with `0600` permissions to protect secrets."
      ],
      "id": "8ChY39vqtgw_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOe3tJPetgxA"
      },
      "source": [
        "### Build Prompt\n",
        "Generate a **single Colab code cell** that:\n",
        "- Prompts me to upload `kaggle.json`,\n",
        "- Saves to `~/.kaggle/kaggle.json` with `0600` permissions,\n",
        "- Prints `kaggle --version`.\n",
        "Add comments about security and reproducibility.\n"
      ],
      "id": "DOe3tJPetgxA"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "FvyfTMLStgxB",
        "outputId": "41650787-83fb-4c08-dfed-b36142faff03"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload your kaggle.json (Kaggle > Account > Create New API Token)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-fa574270-8871-4c8a-a6ec-81bedf58c784\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-fa574270-8871-4c8a-a6ec-81bedf58c784\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Kaggle API 1.7.4.5\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "print(\"Upload your kaggle.json (Kaggle > Account > Create New API Token)\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "import os\n",
        "os.makedirs('/root/.kaggle', exist_ok=True)\n",
        "with open('/root/.kaggle/kaggle.json', 'wb') as f:\n",
        "    f.write(uploaded[list(uploaded.keys())[0]])\n",
        "os.chmod('/root/.kaggle/kaggle.json', 0o600)  # owner-only\n",
        "\n",
        "!kaggle --version"
      ],
      "id": "FvyfTMLStgxB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neiWKTnwtgxB"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a one-liner that runs `kaggle --help | head -n 20` to show the CLI is ready.\n"
      ],
      "id": "neiWKTnwtgxB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eZMAFT6tgxB"
      },
      "source": [
        "**Reflection:** Why require strict `0600` permissions on API tokens? What risks are we avoiding?\n",
        "\n",
        "*Requiring strict 0600 permissions on API tokens like kaggle.json is very important for security. This setting allows only the file's owner to read and write to it, preventing unauthorized access by other users on the system. By limiting access, we significantly reduce the risk of exposing sensitive credentials that could be used to access or misuse your accounts and data. It's a standard practice to protect against potential data breaches and unauthorized actions.*"
      ],
      "id": "3eZMAFT6tgxB"
    },
    {
      "cell_type": "code",
      "source": [
        "# Verification step: Show Kaggle CLI help output\n",
        "!kaggle --help | head -n 20"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qa2lUoACyz7A",
        "outputId": "f45f05ab-06b9-4a78-8524-aee6593855f8"
      },
      "id": "qa2lUoACyz7A",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: kaggle [-h] [-v] [-W]\n",
            "              {competitions,c,datasets,d,kernels,k,models,m,files,f,config}\n",
            "              ...\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  -v, --version         Print the Kaggle API version\n",
            "  -W, --no-warn         Disable out-of-date API version warning\n",
            "\n",
            "commands:\n",
            "  {competitions,c,datasets,d,kernels,k,models,m,files,f,config}\n",
            "                        Use one of:\n",
            "                        competitions {list, files, download, submit, submissions, leaderboard}\n",
            "                        datasets {list, files, download, create, version, init, metadata, status}\n",
            "                        kernels {list, files, init, push, pull, output, status}\n",
            "                        models {instances, get, list, init, create, delete, update}\n",
            "                        models instances {versions, get, files, init, create, delete, update}\n",
            "                        models instances versions {init, create, download, delete, files}\n",
            "                        config {view, set, unset}\n",
            "    competitions (c)    Commands related to Kaggle competitions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KcaIeNZtgxC"
      },
      "source": [
        "## 2) Download & unzip dataset — What & Why\n",
        "Keep raw files under `/content/data/raw` for predictable paths and auditing.\n",
        "**Dataset:** `sayeeduddin/netflix-2025user-behavior-dataset-210k-records`"
      ],
      "id": "8KcaIeNZtgxC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brQ8GkOhtgxC"
      },
      "source": [
        "### Build Prompt\n",
        "Generate a **Colab code cell** that:\n",
        "- Creates `/content/data/raw`,\n",
        "- Downloads the dataset to `/content/data` with Kaggle CLI,\n",
        "- Unzips into `/content/data/raw` (overwrite OK),\n",
        "- Lists all CSVs with sizes in a neat table.\n",
        "Include comments describing each step.\n"
      ],
      "id": "brQ8GkOhtgxC"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b91xG0WRtgxC",
        "outputId": "e2258a1c-b743-45d6-dc5c-e8e03990e23a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/sayeeduddin/netflix-2025user-behavior-dataset-210k-records\n",
            "License(s): CC0-1.0\n",
            "Downloading netflix-2025user-behavior-dataset-210k-records.zip to /content/data\n",
            "  0% 0.00/4.02M [00:00<?, ?B/s]\n",
            "100% 4.02M/4.02M [00:00<00:00, 558MB/s]\n",
            "Archive:  /content/data/netflix-2025user-behavior-dataset-210k-records.zip\n",
            "  inflating: /content/data/raw/README.md  \n",
            "  inflating: /content/data/raw/movies.csv  \n",
            "  inflating: /content/data/raw/recommendation_logs.csv  \n",
            "  inflating: /content/data/raw/reviews.csv  \n",
            "  inflating: /content/data/raw/search_logs.csv  \n",
            "  inflating: /content/data/raw/users.csv  \n",
            "  inflating: /content/data/raw/watch_history.csv  \n",
            "-rw-r--r-- 1 root root 114K Aug  2 19:36 /content/data/raw/movies.csv\n",
            "-rw-r--r-- 1 root root 4.5M Aug  2 19:36 /content/data/raw/recommendation_logs.csv\n",
            "-rw-r--r-- 1 root root 1.8M Aug  2 19:36 /content/data/raw/reviews.csv\n",
            "-rw-r--r-- 1 root root 2.2M Aug  2 19:36 /content/data/raw/search_logs.csv\n",
            "-rw-r--r-- 1 root root 1.6M Aug  2 19:36 /content/data/raw/users.csv\n",
            "-rw-r--r-- 1 root root 8.9M Aug  2 19:36 /content/data/raw/watch_history.csv\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p /content/data/raw\n",
        "!kaggle datasets download -d sayeeduddin/netflix-2025user-behavior-dataset-210k-records -p /content/data\n",
        "!unzip -o /content/data/*.zip -d /content/data/raw\n",
        "# List CSV inventory\n",
        "!ls -lh /content/data/raw/*.csv"
      ],
      "id": "b91xG0WRtgxC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hg5nexkhtgxC"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a snippet that asserts there are exactly **six** CSV files and prints their names.\n"
      ],
      "id": "Hg5nexkhtgxC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cNeN6uptgxD"
      },
      "source": [
        "**Reflection:** Why is keeping a clean file inventory (names, sizes) useful downstream?\n",
        "\n",
        "*Keeping a clean file inventory with names and sizes is useful downstream for several reasons/purposes. It provides a clear record of the raw data available, which is essential for reproducibility and auditing. Knowing the file sizes helps in estimating storage needs and planning data processing tasks. It also makes it easier to locate specific files and verify that all expected data is present before proceeding with analysis or loading into other systems.*"
      ],
      "id": "5cNeN6uptgxD"
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "csv_files = glob.glob('/content/data/raw/*.csv')\n",
        "assert len(csv_files) == 6, f\"Expected 6 CSV files, but found {len(csv_files)}\"\n",
        "\n",
        "print(\"Found 6 CSV files:\")\n",
        "for csv_file in csv_files:\n",
        "    print(os.path.basename(csv_file))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kxlDoxFy5X4",
        "outputId": "1c378533-ad46-4195-9080-463e10f7009d"
      },
      "id": "7kxlDoxFy5X4",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6 CSV files:\n",
            "movies.csv\n",
            "watch_history.csv\n",
            "search_logs.csv\n",
            "users.csv\n",
            "recommendation_logs.csv\n",
            "reviews.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTHltEzttgxD"
      },
      "source": [
        "## 3) Create GCS bucket & upload — What & Why\n",
        "Stage in GCS → consistent, versionable source for BigQuery loads. Bucket names must be **globally unique**."
      ],
      "id": "WTHltEzttgxD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Akgamu8tgxD"
      },
      "source": [
        "### Build Prompt\n",
        "Generate a **Colab code cell** that:\n",
        "- Creates a unique bucket in `${REGION}` (random suffix),\n",
        "- Saves name to `BUCKET_NAME` env var,\n",
        "- Uploads all CSVs to `gs://$BUCKET_NAME/netflix/`,\n",
        "- Prints the bucket name and explains staging benefits.\n"
      ],
      "id": "5Akgamu8tgxD"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5EB21ditgxD",
        "outputId": "a8647cf7-aa40-4268-cf8a-64deeac0c38c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating gs://mgmt467-netflix-faa7c31d/...\n",
            "Copying file:///content/data/raw/movies.csv to gs://mgmt467-netflix-faa7c31d/netflix/movies.csv\n",
            "Copying file:///content/data/raw/README.md to gs://mgmt467-netflix-faa7c31d/netflix/README.md\n",
            "Copying file:///content/data/raw/recommendation_logs.csv to gs://mgmt467-netflix-faa7c31d/netflix/recommendation_logs.csv\n",
            "Copying file:///content/data/raw/reviews.csv to gs://mgmt467-netflix-faa7c31d/netflix/reviews.csv\n",
            "Copying file:///content/data/raw/search_logs.csv to gs://mgmt467-netflix-faa7c31d/netflix/search_logs.csv\n",
            "Copying file:///content/data/raw/users.csv to gs://mgmt467-netflix-faa7c31d/netflix/users.csv\n",
            "Copying file:///content/data/raw/watch_history.csv to gs://mgmt467-netflix-faa7c31d/netflix/watch_history.csv\n",
            "\n",
            "Average throughput: 14.4MiB/s\n",
            "Bucket: mgmt467-netflix-faa7c31d\n",
            "gs://mgmt467-netflix-faa7c31d/netflix/README.md\n",
            "gs://mgmt467-netflix-faa7c31d/netflix/movies.csv\n",
            "gs://mgmt467-netflix-faa7c31d/netflix/recommendation_logs.csv\n",
            "gs://mgmt467-netflix-faa7c31d/netflix/reviews.csv\n",
            "gs://mgmt467-netflix-faa7c31d/netflix/search_logs.csv\n",
            "gs://mgmt467-netflix-faa7c31d/netflix/users.csv\n",
            "gs://mgmt467-netflix-faa7c31d/netflix/watch_history.csv\n"
          ]
        }
      ],
      "source": [
        "import uuid, os\n",
        "bucket_name = f\"mgmt467-netflix-{uuid.uuid4().hex[:8]}\"\n",
        "os.environ[\"BUCKET_NAME\"] = bucket_name\n",
        "!gcloud storage buckets create gs://$BUCKET_NAME --location=US\n",
        "!gcloud storage cp /content/data/raw/* gs://$BUCKET_NAME/netflix/\n",
        "print(\"Bucket:\", bucket_name)\n",
        "# Verify contents\n",
        "!gcloud storage ls gs://$BUCKET_NAME/netflix/"
      ],
      "id": "U5EB21ditgxD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJjNivOutgxD"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a snippet that lists the `netflix/` prefix and shows object sizes.\n"
      ],
      "id": "aJjNivOutgxD"
    },
    {
      "cell_type": "code",
      "source": [
        "# Verification step: List objects in the netflix/ prefix with sizes\n",
        "import os\n",
        "bucket_name = os.environ.get(\"BUCKET_NAME\")\n",
        "if bucket_name:\n",
        "  !gcloud storage ls -l gs://$BUCKET_NAME/netflix/\n",
        "else:\n",
        "  print(\"BUCKET_NAME environment variable not set.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5ksH7Dp1QWu",
        "outputId": "d398ae0b-186b-4b42-ba3c-30ef06af34a3"
      },
      "id": "g5ksH7Dp1QWu",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      8002  2025-10-27T03:34:55Z  gs://mgmt467-netflix-faa7c31d/netflix/README.md\n",
            "    115942  2025-10-27T03:34:55Z  gs://mgmt467-netflix-faa7c31d/netflix/movies.csv\n",
            "   4695557  2025-10-27T03:34:55Z  gs://mgmt467-netflix-faa7c31d/netflix/recommendation_logs.csv\n",
            "   1861942  2025-10-27T03:34:56Z  gs://mgmt467-netflix-faa7c31d/netflix/reviews.csv\n",
            "   2250902  2025-10-27T03:34:56Z  gs://mgmt467-netflix-faa7c31d/netflix/search_logs.csv\n",
            "   1606820  2025-10-27T03:34:55Z  gs://mgmt467-netflix-faa7c31d/netflix/users.csv\n",
            "   9269425  2025-10-27T03:34:56Z  gs://mgmt467-netflix-faa7c31d/netflix/watch_history.csv\n",
            "TOTAL: 7 objects, 19808590 bytes (18.89MiB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ykvTReNtgxE"
      },
      "source": [
        "**Reflection:** Name two benefits of staging in GCS vs loading directly from local Colab.\n",
        "\n",
        "*Two benefits of staging data in GCS compared to loading directly from local Colab are consistency and scalability. GCS provides a stable, central location for data that can be easily accessed by various services, unlike transient local Colab storage. Additionally, GCS is designed for scalability, handling large datasets efficiently, which is crucial for big data processing in BigQuery.*"
      ],
      "id": "4ykvTReNtgxE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGjO6rautgxE"
      },
      "source": [
        "## 4) BigQuery dataset & loads — What & Why\n",
        "Create dataset `netflix` and load six CSVs with **autodetect** for speed (we’ll enforce schemas later)."
      ],
      "id": "bGjO6rautgxE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBP8VOSitgxE"
      },
      "source": [
        "### Build Prompt (two cells)\n",
        "**Cell A:** Create (idempotently) dataset `netflix` in US multi-region; if it exists, print a friendly message.  \n",
        "**Cell B:** Load tables from `gs://$BUCKET_NAME/netflix/`:\n",
        "`users, movies, watch_history, recommendation_logs, search_logs, reviews`\n",
        "with `--skip_leading_rows=1 --autodetect --source_format=CSV`.\n",
        "Finish with row-count queries for each table.\n"
      ],
      "id": "GBP8VOSitgxE"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWkahTEdtgxE",
        "outputId": "d621e259-0989-4dd2-c5fa-36f31461574a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BigQuery error in mk operation: Dataset 'mgmt467-project1:netflix' already\n",
            "exists.\n",
            "Dataset may already exist.\n"
          ]
        }
      ],
      "source": [
        "DATASET=\"netflix\"\n",
        "# Attempt to create; ignore if exists\n",
        "!bq --location=US mk -d --description \"MGMT467 Netflix dataset\" $DATASET || echo \"Dataset may already exist.\""
      ],
      "id": "rWkahTEdtgxE"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ColdY1PWtgxE",
        "outputId": "4afa040a-1191-43cd-cdc2-2f00a777dc5f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading users from gs://mgmt467-netflix-faa7c31d/netflix/users.csv\n",
            "Waiting on bqjob_r323266820e8be52_0000019a23bf6062_1 ... (1s) Current status: DONE   \n",
            "Loading movies from gs://mgmt467-netflix-faa7c31d/netflix/movies.csv\n",
            "Waiting on bqjob_r652923419485a191_0000019a23bf7dd7_1 ... (1s) Current status: DONE   \n",
            "Loading watch_history from gs://mgmt467-netflix-faa7c31d/netflix/watch_history.csv\n",
            "Waiting on bqjob_r59285c6f23403e11_0000019a23bf9b61_1 ... (2s) Current status: DONE   \n",
            "Loading recommendation_logs from gs://mgmt467-netflix-faa7c31d/netflix/recommendation_logs.csv\n",
            "Waiting on bqjob_r36e7615b95d85afd_0000019a23bfbd66_1 ... (1s) Current status: DONE   \n",
            "Loading search_logs from gs://mgmt467-netflix-faa7c31d/netflix/search_logs.csv\n",
            "Waiting on bqjob_r686f5d3f48455020_0000019a23bfda01_1 ... (2s) Current status: DONE   \n",
            "Loading reviews from gs://mgmt467-netflix-faa7c31d/netflix/reviews.csv\n",
            "Waiting on bqjob_r183b58846911b048_0000019a23bffb9c_1 ... (1s) Current status: DONE   \n",
            "/bin/bash: line 1: mgmt467-project1.netflix.users: command not found\n",
            "Error in query string: Error processing job\n",
            "'mgmt467-project1:bqjob_r11285715ea59dd7d_0000019a23c01835_1': Syntax error:\n",
            "Unexpected end of script at [1:49]\n",
            "/bin/bash: line 1: mgmt467-project1.netflix.movies: command not found\n",
            "Error in query string: Error processing job\n",
            "'mgmt467-project1:bqjob_r3c29399a01b7daa_0000019a23c02a96_1': Syntax error:\n",
            "Unexpected end of script at [1:50]\n",
            "/bin/bash: line 1: mgmt467-project1.netflix.watch_history: command not found\n",
            "Error in query string: Error processing job\n",
            "'mgmt467-project1:bqjob_r314f13a2ef2f5dc9_0000019a23c03bc2_1': Syntax error:\n",
            "Unexpected end of script at [1:57]\n",
            "/bin/bash: line 1: mgmt467-project1.netflix.recommendation_logs: command not found\n",
            "Error in query string: Error processing job\n",
            "'mgmt467-project1:bqjob_r2c7ad3b0051616e_0000019a23c04eba_1': Syntax error:\n",
            "Unexpected end of script at [1:63]\n",
            "/bin/bash: line 1: mgmt467-project1.netflix.search_logs: command not found\n",
            "Error in query string: Error processing job\n",
            "'mgmt467-project1:bqjob_r1c935db6ac7fff94_0000019a23c05fb5_1': Syntax error:\n",
            "Unexpected end of script at [1:55]\n",
            "/bin/bash: line 1: mgmt467-project1.netflix.reviews: command not found\n",
            "Error in query string: Error processing job\n",
            "'mgmt467-project1:bqjob_r2523a6013dd4f1fa_0000019a23c070f3_1': Syntax error:\n",
            "Unexpected end of script at [1:51]\n"
          ]
        }
      ],
      "source": [
        "tables = {\n",
        "  \"users\": \"users.csv\",\n",
        "  \"movies\": \"movies.csv\",\n",
        "  \"watch_history\": \"watch_history.csv\",\n",
        "  \"recommendation_logs\": \"recommendation_logs.csv\",\n",
        "  \"search_logs\": \"search_logs.csv\",\n",
        "  \"reviews\": \"reviews.csv\",\n",
        "}\n",
        "import os\n",
        "for tbl, fname in tables.items():\n",
        "  src = f\"gs://{os.environ['BUCKET_NAME']}/netflix/{fname}\"\n",
        "  print(\"Loading\", tbl, \"from\", src)\n",
        "  !bq load --skip_leading_rows=1 --autodetect --source_format=CSV {DATASET}.{tbl} {src}\n",
        "\n",
        "# Row counts\n",
        "for tbl in tables.keys():\n",
        "  # Use correct shell command formatting\n",
        "  !bq query --nouse_legacy_sql \"SELECT '{tbl}' AS table_name, COUNT(*) AS n FROM `{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.{tbl}`\""
      ],
      "id": "ColdY1PWtgxE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUmePBGAtgxE"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a single query that returns `table_name, row_count` for all six tables in `${GOOGLE_CLOUD_PROJECT}.netflix`.\n"
      ],
      "id": "QUmePBGAtgxE"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0ba4b70",
        "outputId": "8d8a3c10-b8ad-4342-a298-be5cd1c6c2a3"
      },
      "source": [
        "from google.cloud import bigquery\n",
        "import os\n",
        "\n",
        "client = bigquery.Client(project=os.environ['GOOGLE_CLOUD_PROJECT'])\n",
        "\n",
        "query = f\"\"\"\n",
        "SELECT 'users' AS table_name, COUNT(*) AS n FROM `{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.users`\n",
        "UNION ALL\n",
        "SELECT 'movies' AS table_name, COUNT(*) AS n FROM `{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.movies`\n",
        "UNION ALL\n",
        "SELECT 'watch_history' AS table_name, COUNT(*) AS n FROM `{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.watch_history`\n",
        "UNION ALL\n",
        "SELECT 'recommendation_logs' AS table_name, COUNT(*) AS n FROM `{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.recommendation_logs`\n",
        "UNION ALL\n",
        "SELECT 'search_logs' AS table_name, COUNT(*) AS n FROM `{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.search_logs`\n",
        "UNION ALL\n",
        "SELECT 'reviews' AS table_name, COUNT(*) AS n FROM `{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.reviews`\n",
        "\"\"\"\n",
        "\n",
        "results = client.query(query).result()\n",
        "\n",
        "for row in results:\n",
        "    print(f\"{row.table_name}: {row.n} rows\")\n",
        "\n",
        "\n"
      ],
      "id": "d0ba4b70",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "movies: 2080 rows\n",
            "users: 20600 rows\n",
            "reviews: 30900 rows\n",
            "watch_history: 210000 rows\n",
            "search_logs: 53000 rows\n",
            "recommendation_logs: 104000 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iMzk47ptgxE"
      },
      "source": [
        "**Reflection:** When is `autodetect` acceptable? When should you enforce explicit schemas and why?\n",
        "\n",
        "*Autodetect is acceptable for initial data exploration or when the schema is simple and not likely to change, as it's quick and convenient. However, you should enforce explicit schemas when data quality and consistency are critical, or when dealing with complex nested structures or evolving data sources. Explicit schemas provide better control, data validation, and prevent unexpected data type conversions or errors during loads, ensuring data integrity for downstream analysis and applications.*"
      ],
      "id": "-iMzk47ptgxE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyB-_ofZtgxE"
      },
      "source": [
        "## 5) Data Quality (DQ) — Concepts we care about\n",
        "- **Missingness** (MCAR/MAR/MNAR). Impute vs drop. Add `is_missing_*` indicators.\n",
        "- **Duplicates** (exact vs near). Double-counted engagement corrupts labels & KPIs.\n",
        "- **Outliers** (IQR). Winsorize/cap vs robust models. Always **flag** and explain.\n",
        "- **Reproducibility**. Prefer `CREATE OR REPLACE` and deterministic keys.\n"
      ],
      "id": "CyB-_ofZtgxE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2Ouqy4RtgxF"
      },
      "source": [
        "### 5.1 Missingness (users) — What & Why\n",
        "Measure % missing and check if missingness depends on another variable (MAR) → potential bias & instability."
      ],
      "id": "q2Ouqy4RtgxF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkbXpq-ttgxF"
      },
      "source": [
        "### Build Prompt\n",
        "Generate **two BigQuery SQL cells**:\n",
        "1) Total rows and % missing in `region`, `plan_tier`, `age_band` from `users`.\n",
        "2) `% plan_tier missing by region` ordered descending. Add comments on MAR.\n"
      ],
      "id": "zkbXpq-ttgxF"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkpvVosXtgxF",
        "outputId": "9e64a039-f70b-4506-e276-d506c0976389"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row((20600, 0.0, 0.0, 11.93), {'n': 0, 'pct_missing_country': 1, 'pct_missing_subscription_plan': 2, 'pct_missing_age': 3})\n"
          ]
        }
      ],
      "source": [
        "from google.cloud import bigquery\n",
        "import os\n",
        "\n",
        "client = bigquery.Client(project=os.environ['GOOGLE_CLOUD_PROJECT'])\n",
        "\n",
        "query_missingness = f\"\"\"\n",
        "WITH base AS (\n",
        "  SELECT COUNT(*) AS n,\n",
        "         COUNTIF(country IS NULL) AS miss_country,\n",
        "         COUNTIF(subscription_plan IS NULL) AS miss_subscription_plan,\n",
        "         COUNTIF(age IS NULL) AS miss_age\n",
        "  FROM `{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.users`\n",
        ")\n",
        "SELECT n,\n",
        "       ROUND(100*miss_country/n,2) AS pct_missing_country,\n",
        "       ROUND(100*miss_subscription_plan/n,2) AS pct_missing_subscription_plan,\n",
        "       ROUND(100*miss_age/n,2) AS pct_missing_age\n",
        "FROM base;\n",
        "\"\"\"\n",
        "\n",
        "results = client.query(query_missingness).result()\n",
        "for row in results:\n",
        "    print(row)\n"
      ],
      "id": "AkpvVosXtgxF"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery\n",
        "import os\n",
        "\n",
        "client = bigquery.Client(project=os.environ['GOOGLE_CLOUD_PROJECT'])\n",
        "\n",
        "query = f\"\"\"\n",
        "SELECT country,\n",
        "       COUNT(*) AS n,\n",
        "       ROUND(100*COUNTIF(subscription_plan IS NULL)/COUNT(*),2) AS pct_missing_subscription_plan\n",
        "FROM `{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.users`\n",
        "GROUP BY country\n",
        "ORDER BY pct_missing_subscription_plan DESC;\n",
        "\"\"\"\n",
        "\n",
        "results = client.query(query).result()\n",
        "\n",
        "for row in results:\n",
        "    print(row)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TZwIGlS71Xw",
        "outputId": "22931251-91cf-4187-c39b-4ac8f246907b"
      },
      "id": "0TZwIGlS71Xw",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row(('Canada', 6192, 0.0), {'country': 0, 'n': 1, 'pct_missing_subscription_plan': 2})\n",
            "Row(('USA', 14408, 0.0), {'country': 0, 'n': 1, 'pct_missing_subscription_plan': 2})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBcSm8DItgxF"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a query that prints the three missingness percentages from (1), rounded to two decimals.\n"
      ],
      "id": "cBcSm8DItgxF"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c95d14ae",
        "outputId": "fada8ce4-30f8-443d-a86c-7251aca500af"
      },
      "source": [
        "# Verification query: Print the three missingness percentages\n",
        "from google.cloud import bigquery\n",
        "import os\n",
        "\n",
        "client = bigquery.Client(project=os.environ['GOOGLE_CLOUD_PROJECT'])\n",
        "project = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "\n",
        "query = f\"\"\"\n",
        "WITH base AS (\n",
        "  SELECT COUNT(*) n,\n",
        "         COUNTIF(country IS NULL) miss_country,\n",
        "         COUNTIF(subscription_plan IS NULL) miss_plan,\n",
        "         COUNTIF(age IS NULL) miss_age\n",
        "  FROM `{project}.netflix.users`\n",
        ")\n",
        "SELECT ROUND(100*miss_country/n,2) AS pct_missing_country,\n",
        "       ROUND(100*miss_plan/n,2) AS pct_missing_plan,\n",
        "       ROUND(100*miss_age/n,2) AS pct_missing_age\n",
        "FROM base;\n",
        "\"\"\"\n",
        "\n",
        "results = client.query(query).result()\n",
        "\n",
        "for row in results:\n",
        "    print(row)\n",
        "\n"
      ],
      "id": "c95d14ae",
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row((0.0, 0.0, 11.93), {'pct_missing_country': 0, 'pct_missing_plan': 1, 'pct_missing_age': 2})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyQV1XPRtgxF"
      },
      "source": [
        "**Reflection:** Which columns are most missing? Hypothesize MCAR/MAR/MNAR and why.\n",
        "\n",
        "*Based on the query results, the age_band column is the most missing, with about 11.93% of values being null. It's possible this is Missing At Random (MAR), where the missingness might be related to another unobserved user characteristic, or Missing Not At Random (MNAR) if users are less likely to provide their age for privacy reasons. region and plan_tier have no missing values, suggesting complete data for those columns.*"
      ],
      "id": "TyQV1XPRtgxF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZPC05R9tgxF"
      },
      "source": [
        "### 5.2 Duplicates (watch_history) — What & Why\n",
        "Find exact duplicate interaction records and keep **one best** per group (deterministic policy)."
      ],
      "id": "hZPC05R9tgxF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWT_nzYjtgxF"
      },
      "source": [
        "### Build Prompt\n",
        "Generate **two BigQuery SQL cells**:\n",
        "1) Report duplicate groups on `(user_id, movie_id, event_ts, device_type)` with counts (top 20).\n",
        "2) Create table `watch_history_dedup` that keeps one row per group (prefer higher `progress_ratio`, then `minutes_watched`). Add comments.\n"
      ],
      "id": "jWT_nzYjtgxF"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery\n",
        "import os\n",
        "\n",
        "client = bigquery.Client(project=os.environ['GOOGLE_CLOUD_PROJECT'])\n",
        "project = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "\n",
        "query = f\"\"\"\n",
        "SELECT user_id, movie_id, watch_date, device_type, COUNT(*) AS dup_count\n",
        "FROM `{project}.netflix.watch_history`\n",
        "GROUP BY user_id, movie_id, watch_date, device_type\n",
        "HAVING dup_count > 1\n",
        "ORDER BY dup_count DESC\n",
        "LIMIT 20\n",
        "\"\"\"\n",
        "\n",
        "results = client.query(query).result()\n",
        "\n",
        "for row in results:\n",
        "    print(row)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbooFmp682qI",
        "outputId": "8eb7d32a-224d-4fed-f41c-08201d4ef6e9"
      },
      "id": "FbooFmp682qI",
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row(('user_00391', 'movie_0893', datetime.date(2024, 8, 26), 'Laptop', 8), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_03310', 'movie_0640', datetime.date(2024, 9, 8), 'Smart TV', 8), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_01383', 'movie_0015', datetime.date(2025, 4, 29), 'Desktop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_03021', 'movie_0602', datetime.date(2025, 2, 23), 'Laptop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_01182', 'movie_0794', datetime.date(2025, 7, 3), 'Desktop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_01807', 'movie_0921', datetime.date(2025, 1, 30), 'Laptop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_01870', 'movie_0844', datetime.date(2024, 6, 2), 'Laptop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_07981', 'movie_0094', datetime.date(2025, 11, 8), 'Laptop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_02652', 'movie_0352', datetime.date(2024, 10, 22), 'Desktop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_09973', 'movie_0342', datetime.date(2025, 3, 22), 'Desktop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_09512', 'movie_0825', datetime.date(2025, 1, 7), 'Desktop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_04027', 'movie_0652', datetime.date(2024, 1, 2), 'Mobile', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_02549', 'movie_0428', datetime.date(2025, 4, 15), 'Mobile', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_03140', 'movie_0205', datetime.date(2025, 9, 11), 'Desktop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_08826', 'movie_0133', datetime.date(2025, 4, 11), 'Desktop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_04050', 'movie_0898', datetime.date(2025, 7, 5), 'Mobile', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_08276', 'movie_0460', datetime.date(2024, 4, 24), 'Desktop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_02950', 'movie_0928', datetime.date(2025, 6, 3), 'Desktop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_08177', 'movie_0593', datetime.date(2025, 1, 15), 'Laptop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_00965', 'movie_0991', datetime.date(2024, 2, 14), 'Desktop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAJHmWpOtgxG",
        "outputId": "3d8c5571-77bd-4d45-d894-bc28407d2e1d"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ watch_history_dedup table created successfully\n"
          ]
        }
      ],
      "source": [
        "from google.cloud import bigquery\n",
        "import os\n",
        "\n",
        "client = bigquery.Client(project=os.environ['GOOGLE_CLOUD_PROJECT'])\n",
        "project = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "\n",
        "# Replace 'watch_date' with your actual timestamp column (or event_ts)\n",
        "timestamp_col = \"watch_date\"\n",
        "\n",
        "query = f\"\"\"\n",
        "CREATE OR REPLACE TABLE `{project}.netflix.watch_history_dedup` AS\n",
        "SELECT * EXCEPT(rk) FROM (\n",
        "  SELECT h.*,\n",
        "         ROW_NUMBER() OVER (\n",
        "           PARTITION BY user_id, movie_id, {timestamp_col}, device_type\n",
        "           ORDER BY {timestamp_col} DESC\n",
        "         ) AS rk\n",
        "  FROM `{project}.netflix.watch_history` h\n",
        ")\n",
        "WHERE rk = 1\n",
        "\"\"\"\n",
        "\n",
        "client.query(query).result()\n",
        "print(\"✓ watch_history_dedup table created successfully\")\n"
      ],
      "id": "cAJHmWpOtgxG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySRo1bYStgxG"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a before/after count query comparing raw vs `watch_history_dedup`.\n"
      ],
      "id": "ySRo1bYStgxG"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d566c2cd",
        "outputId": "c10e15c1-b81a-45e8-ee16-12d98a8e1533"
      },
      "source": [
        "from google.cloud import bigquery\n",
        "import os\n",
        "\n",
        "# Set project\n",
        "project = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project)\n",
        "\n",
        "# SQL query: row counts for raw and dedup tables\n",
        "sql = f\"\"\"\n",
        "SELECT 'watch_history_raw' AS table_name, COUNT(*) AS row_count\n",
        "FROM `{project}.netflix.watch_history`\n",
        "UNION ALL\n",
        "SELECT 'watch_history_dedup' AS table_name, COUNT(*) AS row_count\n",
        "FROM `{project}.netflix.watch_history_dedup`\n",
        "\"\"\"\n",
        "\n",
        "# Run the query\n",
        "results = client.query(sql).result()\n",
        "\n",
        "# Print results\n",
        "for row in results:\n",
        "    print(row.table_name, row.row_count)\n"
      ],
      "id": "d566c2cd",
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "watch_history_dedup 100000\n",
            "watch_history_raw 210000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFz7TKLdtgxG"
      },
      "source": [
        "**Reflection:** Why do duplicates arise (natural vs system-generated)? How do they corrupt labels and KPIs?\n",
        "\n",
        "*Duplicates can arise naturally from user actions (e.g., accidentally submitting a form twice) or be system-generated due to errors in data pipelines or logging. They corrupt labels and KPIs by artificially inflating counts, such as the number of views or interactions, leading to inaccurate analysis and potentially flawed business decisions or machine learning model training. Deduplication is crucial for ensuring data integrity and reliable metrics.*"
      ],
      "id": "dFz7TKLdtgxG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHmVAdTZtgxG"
      },
      "source": [
        "### 5.3 Outliers (minutes_watched) — What & Why\n",
        "Estimate extreme values via IQR; report % outliers; **winsorize** to P01/P99 for robustness while also **flagging** extremes."
      ],
      "id": "HHmVAdTZtgxG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTwgVxjdtgxG"
      },
      "source": [
        "### Build Prompt\n",
        "Generate **two BigQuery SQL cells**:\n",
        "1) Compute IQR bounds for `minutes_watched` on `watch_history_dedup` and report % outliers.\n",
        "2) Create `watch_history_robust` with `minutes_watched_capped` capped at P01/P99; return quantile summaries before/after.\n"
      ],
      "id": "hTwgVxjdtgxG"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9q-hC0r4tgxG",
        "outputId": "b7f974bd-a162-4d75-8b85-6451854831ec"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row((3472, 100000, 3.47), {'outliers': 0, 'total': 1, 'pct_outliers': 2})\n"
          ]
        }
      ],
      "source": [
        "from google.cloud import bigquery\n",
        "import os\n",
        "\n",
        "client = bigquery.Client(project=os.environ['GOOGLE_CLOUD_PROJECT'])\n",
        "project = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "\n",
        "sql = f\"\"\"\n",
        "WITH dist AS (\n",
        "  SELECT\n",
        "    APPROX_QUANTILES(watch_duration_minutes, 4)[OFFSET(1)] AS q1,\n",
        "    APPROX_QUANTILES(watch_duration_minutes, 4)[OFFSET(3)] AS q3\n",
        "  FROM `{project}.netflix.watch_history_dedup`\n",
        "),\n",
        "bounds AS (\n",
        "  SELECT q1, q3, (q3-q1) AS iqr,\n",
        "         q1 - 1.5*(q3-q1) AS lo,\n",
        "         q3 + 1.5*(q3-q1) AS hi\n",
        "  FROM dist\n",
        ")\n",
        "SELECT\n",
        "  COUNTIF(h.watch_duration_minutes < b.lo OR h.watch_duration_minutes > b.hi) AS outliers,\n",
        "  COUNT(*) AS total,\n",
        "  ROUND(100*COUNTIF(h.watch_duration_minutes < b.lo OR h.watch_duration_minutes > b.hi)/COUNT(*),2) AS pct_outliers\n",
        "FROM `{project}.netflix.watch_history_dedup` h\n",
        "CROSS JOIN bounds b;\n",
        "\"\"\"\n",
        "\n",
        "results = client.query(sql).result()\n",
        "\n",
        "for row in results:\n",
        "    print(row)\n"
      ],
      "id": "9q-hC0r4tgxG"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFsGL2vftgxG",
        "outputId": "89591df9-48ae-4df0-d9c0-966b238b433d"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ watch_history_robust table created successfully\n"
          ]
        }
      ],
      "source": [
        "from google.cloud import bigquery\n",
        "import os\n",
        "\n",
        "client = bigquery.Client(project=os.environ['GOOGLE_CLOUD_PROJECT'])\n",
        "project = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "\n",
        "sql_create_robust = f\"\"\"\n",
        "CREATE OR REPLACE TABLE `{project}.netflix.watch_history_robust` AS\n",
        "SELECT *\n",
        "FROM `{project}.netflix.watch_history_dedup`\n",
        "WHERE watch_duration_minutes BETWEEN 0 AND 500;  -- example: keep only reasonable values\n",
        "\"\"\"\n",
        "\n",
        "# Run the query\n",
        "client.query(sql_create_robust).result()\n",
        "print(\"✓ watch_history_robust table created successfully\")\n"
      ],
      "id": "jFsGL2vftgxG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51VI5RCQtgxH"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a query that shows min/median/max before vs after capping.\n"
      ],
      "id": "51VI5RCQtgxH"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "591632b4",
        "outputId": "96e0aa45-a064-4adc-d4cb-9561fa7ce773"
      },
      "source": [
        "# Query to show min/median/max before vs after capping\n",
        "from google.cloud import bigquery\n",
        "import os\n",
        "\n",
        "client = bigquery.Client(project=os.environ['GOOGLE_CLOUD_PROJECT'])\n",
        "project = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "\n",
        "sql = f\"\"\"\n",
        "SELECT\n",
        "  'Before Capping' AS source,\n",
        "  MIN(watch_duration_minutes) AS min_duration,\n",
        "  APPROX_QUANTILES(watch_duration_minutes, 2)[OFFSET(1)] AS median_duration,\n",
        "  MAX(watch_duration_minutes) AS max_duration\n",
        "FROM `{project}.netflix.watch_history_dedup`\n",
        "UNION ALL\n",
        "SELECT\n",
        "  'After Capping' AS source,\n",
        "  MIN(watch_duration_minutes) AS min_duration,\n",
        "  APPROX_QUANTILES(watch_duration_minutes, 2)[OFFSET(1)] AS median_duration,\n",
        "  MAX(watch_duration_minutes) AS max_duration\n",
        "FROM `{project}.netflix.watch_history_robust`;\n",
        "\"\"\"\n",
        "\n",
        "results = client.query(sql).result()\n",
        "\n",
        "for row in results:\n",
        "    print(row)\n"
      ],
      "id": "591632b4",
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row(('Before Capping', 0.2, 51.2, 799.3), {'source': 0, 'min_duration': 1, 'median_duration': 2, 'max_duration': 3})\n",
            "Row(('After Capping', 0.2, 51.3, 499.5), {'source': 0, 'min_duration': 1, 'median_duration': 2, 'max_duration': 3})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuD051CQtgxH"
      },
      "source": [
        "**Reflection:** When might capping be harmful? Name a model type less sensitive to outliers and why.\n",
        "\n",
        "*Capping might be harmful if the \"outliers\" represent genuine, important variations in the data that are critical for the analysis or model. Removing or altering them could lead to a loss of valuable information. Tree-based models like Decision Trees or Random Forests are generally less sensitive to outliers because they make decisions based on splitting data at certain thresholds rather than being influenced by the magnitude of extreme values like linear models are.*"
      ],
      "id": "fuD051CQtgxH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkHrrDc0tgxH"
      },
      "source": [
        "### 5.4 Business anomaly flags — What & Why\n",
        "Human-readable flags help both product decisioning and ML features (e.g., binge behavior)."
      ],
      "id": "IkHrrDc0tgxH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3CjVnwytgxH"
      },
      "source": [
        "### Build Prompt\n",
        "Generate **three BigQuery SQL cells** (adjust if columns differ):\n",
        "1) In `watch_history_robust`, compute and summarize `flag_binge` for sessions > 8 hours.\n",
        "2) In `users`, compute and summarize `flag_age_extreme` if age can be parsed from `age_band` (<10 or >100).\n",
        "3) In `movies`, compute and summarize `flag_duration_anomaly` where `duration_min` < 15 or > 480 (if exists).\n",
        "Each cell should output count and percentage and include 1–2 comments.\n"
      ],
      "id": "l3CjVnwytgxH"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoemYm47tgxH",
        "outputId": "18c4a2fd-e26f-489f-83df-5231ce1ab156"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row((51, 87641, 0.06), {'sessions_over_8h': 0, 'total': 1, 'pct': 2})\n"
          ]
        }
      ],
      "source": [
        "from google.cloud import bigquery\n",
        "import os\n",
        "\n",
        "client = bigquery.Client(project=os.environ['GOOGLE_CLOUD_PROJECT'])\n",
        "project = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "\n",
        "sql = f\"\"\"\n",
        "SELECT\n",
        "  COUNTIF(watch_duration_minutes > 8*60) AS sessions_over_8h,\n",
        "  COUNT(*) AS total,\n",
        "  ROUND(100*COUNTIF(watch_duration_minutes > 8*60)/COUNT(*),2) AS pct\n",
        "FROM `{project}.netflix.watch_history_robust`;\n",
        "\"\"\"\n",
        "\n",
        "results = client.query(sql).result()\n",
        "\n",
        "for row in results:\n",
        "    print(row)\n"
      ],
      "id": "KoemYm47tgxH"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clfN9l-vtgxH",
        "outputId": "8de65388-21c3-4c54-e3fe-174773c36035"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row((358, 20600, 1.74), {'extreme_age_rows': 0, 'total': 1, 'pct': 2})\n"
          ]
        }
      ],
      "source": [
        "from google.cloud import bigquery\n",
        "import os\n",
        "\n",
        "client = bigquery.Client(project=os.environ['GOOGLE_CLOUD_PROJECT'])\n",
        "project = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "\n",
        "sql = f\"\"\"\n",
        "SELECT\n",
        "  COUNTIF(age < 10 OR age > 100) AS extreme_age_rows,\n",
        "  COUNT(*) AS total,\n",
        "  ROUND(100*COUNTIF(age < 10 OR age > 100)/COUNT(*),2) AS pct\n",
        "FROM `{project}.netflix.users`;\n",
        "\"\"\"\n",
        "\n",
        "results = client.query(sql).result()\n",
        "\n",
        "for row in results:\n",
        "    print(row)\n"
      ],
      "id": "clfN9l-vtgxH"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BW6xBX6YtgxH",
        "outputId": "18781c46-95d1-4aab-a574-7bc3bcaa9e3c"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row((24, 22, 2080, 1.15, 1.06), {'titles_under_15m': 0, 'titles_over_8h': 1, 'total': 2, 'pct_under_15m': 3, 'pct_over_8h': 4})\n"
          ]
        }
      ],
      "source": [
        "from google.cloud import bigquery\n",
        "import os\n",
        "\n",
        "client = bigquery.Client(project=os.environ['GOOGLE_CLOUD_PROJECT'])\n",
        "project = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "\n",
        "sql = f\"\"\"\n",
        "SELECT\n",
        "  COUNTIF(duration_minutes < 15) AS titles_under_15m,\n",
        "  COUNTIF(duration_minutes > 8*60) AS titles_over_8h,\n",
        "  COUNT(*) AS total,\n",
        "  ROUND(100*COUNTIF(duration_minutes < 15)/COUNT(*),2) AS pct_under_15m,\n",
        "  ROUND(100*COUNTIF(duration_minutes > 8*60)/COUNT(*),2) AS pct_over_8h\n",
        "FROM `{project}.netflix.movies`;\n",
        "\"\"\"\n",
        "\n",
        "results = client.query(sql).result()\n",
        "\n",
        "for row in results:\n",
        "    print(row)\n"
      ],
      "id": "BW6xBX6YtgxH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-i3cMSqtgxH"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a single compact summary query that returns two columns per flag: `flag_name, pct_of_rows`.\n"
      ],
      "id": "5-i3cMSqtgxH"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8fe927c",
        "outputId": "bbec90f2-6378-410d-d4a9-c481c2803e6a"
      },
      "source": [
        "from google.cloud import bigquery\n",
        "import os\n",
        "\n",
        "project = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project)\n",
        "\n",
        "sql = f\"\"\"\n",
        "SELECT 'flag_binge' AS flag_name,\n",
        "       ROUND(100*COUNTIF(watch_duration_minutes > 8*60)/COUNT(*),2) AS pct_of_rows\n",
        "FROM `{project}.netflix.watch_history_robust`\n",
        "\n",
        "UNION ALL\n",
        "SELECT 'flag_age_extreme' AS flag_name,\n",
        "       ROUND(100*COUNTIF(age < 10 OR age > 100)/COUNT(*),2) AS pct_of_rows\n",
        "FROM `{project}.netflix.users`\n",
        "\n",
        "UNION ALL\n",
        "SELECT 'flag_duration_anomaly_under_15m' AS flag_name,\n",
        "       ROUND(100*COUNTIF(duration_minutes < 15)/COUNT(*),2) AS pct_of_rows\n",
        "FROM `{project}.netflix.movies`\n",
        "\n",
        "UNION ALL\n",
        "SELECT 'flag_duration_anomaly_over_8h' AS flag_name,\n",
        "       ROUND(100*COUNTIF(duration_minutes > 8*60)/COUNT(*),2) AS pct_of_rows\n",
        "FROM `{project}.netflix.movies`;\n",
        "\"\"\"\n",
        "\n",
        "results = client.query(sql).result()\n",
        "\n",
        "for row in results:\n",
        "    print(row)\n"
      ],
      "id": "e8fe927c",
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row(('flag_binge', 0.06), {'flag_name': 0, 'pct_of_rows': 1})\n",
            "Row(('flag_age_extreme', 1.74), {'flag_name': 0, 'pct_of_rows': 1})\n",
            "Row(('flag_duration_anomaly_under_15m', 1.15), {'flag_name': 0, 'pct_of_rows': 1})\n",
            "Row(('flag_duration_anomaly_over_8h', 1.06), {'flag_name': 0, 'pct_of_rows': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ss1-VufEtgxH"
      },
      "source": [
        "**Reflection:** Which anomaly flag is most common? Which would you keep as a feature and why?\n",
        "\n",
        "*Based on the results, flag_age_extreme is the most common anomaly flag at 1.74%. Whether to keep it as a feature depends on the modeling goal; if age is a strong predictor and these extremes are valid data points, flagging them could help the model. However, if they represent data entry errors, it might be better to treat them differently or exclude them.*"
      ],
      "id": "ss1-VufEtgxH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCI68F8XtgxI"
      },
      "source": [
        "## 6) Save & submit — What & Why\n",
        "Reproducibility: save artifacts and document decisions so others can rerun and audit."
      ],
      "id": "zCI68F8XtgxI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WN9Q-xExtgxI"
      },
      "source": [
        "### Build Prompt\n",
        "Generate a checklist (Markdown) students can paste at the end:\n",
        "- Save this notebook to the team Drive.\n",
        "- Export a `.sql` file with your DQ queries and save to repo.\n",
        "- Push notebook + SQL to the **team GitHub** with a descriptive commit.\n",
        "- Add a README with your `PROJECT_ID`, `REGION`, bucket, dataset, and today’s row counts.\n"
      ],
      "id": "WN9Q-xExtgxI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a103cb45"
      },
      "source": [
        "### Next Steps Checklist:\n",
        "\n",
        "- [ ] Save this notebook to the team Drive.\n",
        "- [ ] Export a `.sql` file with your DQ queries and save to repo.\n",
        "- [ ] Push notebook + SQL to the team GitHub with a descriptive commit.\n",
        "- [ ] Add a README with your `PROJECT_ID`, `REGION`, bucket, dataset, and today’s row counts."
      ],
      "id": "a103cb45"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c1fcf6b"
      },
      "source": [
        "## Project and Data Information\n",
        "\n",
        "*   **Project ID:** mgmt467-project1\n",
        "*   **Region:** us-central1\n",
        "*   **GCS Bucket:** mgmt467-netflix-faa7c31d\n",
        "*   **BigQuery Dataset:** mgmt467-project1.netflix\n",
        "\n",
        "### Row Counts (from today's run)\n",
        "\n",
        "*   **movies:** 2080 rows\n",
        "*   **users:** 20600 rows\n",
        "*   **reviews:** 30900 rows\n",
        "*   **watch_history:** 210000 rows\n",
        "*   **search_logs:** 53000 rows\n",
        "*   **recommendation_logs:** 104000 rows"
      ],
      "id": "0c1fcf6b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEp6PmfjtgxI"
      },
      "source": [
        "## Grading rubric (quick)\n",
        "- Profiling completeness (30)  \n",
        "- Cleaning policy correctness & reproducibility (40)  \n",
        "- Reflection/insight (20)  \n",
        "- Hygiene (naming, verification, idempotence) (10)\n"
      ],
      "id": "KEp6PmfjtgxI"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}